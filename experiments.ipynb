{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "1. Get input from users for the topic\n",
    "2. Get top Google serp results for that \n",
    "3. Scrap text\n",
    "4. Clean text\n",
    "5. Perform keywords extraction\n",
    "6. Keep the above task running every 1 day\n",
    "\n",
    "7. Seperate dashboard to check this online\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the modules\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1: user input from input.xlsx file\n",
    "df=pd.read_excel(\"input.xlsx\",sheet_name=\"input\")\n",
    "inputs = df['topics'].to_list()\n",
    "url_to_fetch_count=df['urls_to_fetch'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyundai https://www.hyundai.com/in/en\n",
      "hyundai https://www.hyundai.com/worldwide/en\n",
      "hyundai http://www.hyundai.com/worldwide/en\n",
      "hyundai https://commons.wikimedia.org/wiki/File:Hyundai_Motor_Company_logo.svg\n",
      "hyundai https://en.wikipedia.org/wiki/Hyundai_Motor_Company\n",
      "Tata https://www.tatamotors.com/\n",
      "Tata http://www.tatamotors.com/\n",
      "Tata https://www.logotaglines.com/tata-motors-logo-taglines/\n",
      "Tata https://en.wikipedia.org/wiki/Tata_Motors\n",
      "Tata https://twitter.com/orfonline/status/1452235926676119552?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Etweet\n"
     ]
    }
   ],
   "source": [
    "## Step 2: Google serp results for inputs making df\n",
    "from googlesearch import search\n",
    "import pandas as pd\n",
    "l=[]\n",
    "for index, inp in enumerate(inputs):\n",
    "    for url in search(inp,tld='com', lang='en', num=10, start=0, stop=url_to_fetch_count[index], pause=2.0,country='India'):\n",
    "        print(inp, url)\n",
    "        l.append(url)\n",
    "\n",
    "df = pd.DataFrame(l,columns=['url'])\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on https://www.hyundai.com/in/en\n",
      "Working on https://www.hyundai.com/worldwide/en\n",
      "Working on http://www.hyundai.com/worldwide/en\n",
      "Working on https://commons.wikimedia.org/wiki/File:Hyundai_Motor_Company_logo.svg\n",
      "Working on https://en.wikipedia.org/wiki/Hyundai_Motor_Company\n",
      "Working on https://www.tatamotors.com/\n",
      "Working on http://www.tatamotors.com/\n",
      "Working on https://www.logotaglines.com/tata-motors-logo-taglines/\n",
      "Working on https://en.wikipedia.org/wiki/Tata_Motors\n",
      "Working on https://twitter.com/orfonline/status/1452235926676119552?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Etweet\n"
     ]
    }
   ],
   "source": [
    "## Step 3: Scraping text for each url\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "def get_page(url):\n",
    "    try:\n",
    "        print(f'Working on {url}')\n",
    "        return BeautifulSoup(requests.get(url).content,'html.parser')\n",
    "    except:\n",
    "        return BeautifulSoup('','html.parser')\n",
    "\n",
    "df['page_content']=df['url'].apply(get_page)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking a sample text\n",
    "# df.iloc[4,1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 4: Text cleaning\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ghost\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "WARNING:root:Candidates are generated using 0.33-top\n"
     ]
    }
   ],
   "source": [
    "## Step 5: Keywords extractions \n",
    "\n",
    "# using pke - https://boudinfl.github.io/pke/build/html/unsupervised.html#textrank\n",
    "import nltk\n",
    "import pke\n",
    "import os\n",
    "os.system('pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz')\n",
    "os.system('python -m spacy link en_core_web_sm en')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# define the set of valid Part-of-Speeches\n",
    "pos = {'NOUN', 'PROPN', 'ADJ'}\n",
    "\n",
    "# 1. create a TextRank extractor.\n",
    "extractor = pke.unsupervised.TextRank()\n",
    "\n",
    "# 2. load the content of the document.\n",
    "extractor.load_document(input=' '.join(df.iloc[:,1].apply(lambda x:x.text).to_list()),\n",
    "                        language='en',\n",
    "                        normalization=None)\n",
    "\n",
    "# 3. build the graph representation of the document and rank the words.\n",
    "#    Keyphrase candidates are composed from the 33-percent\n",
    "#    highest-ranked words.\n",
    "extractor.candidate_weighting(window=2,\n",
    "                              pos=pos,\n",
    "                              top_percent=0.33)\n",
    "\n",
    "# 4. get the 10-highest scored candidates as keyphrases\n",
    "keyphrases = extractor.get_n_best(n=20)\n",
    "\n",
    "# keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote to file!!\n"
     ]
    }
   ],
   "source": [
    "# Writing to output file\n",
    "\n",
    "#loading to dataframe\n",
    "output_df=pd.DataFrame(keyphrases,columns=['keyphrases','text_rank_score'])\n",
    "\n",
    "# saving in the ouput excel file\n",
    "output_df.to_excel(\"output.xlsx\",sheet_name=\"output_keyphrases\",index=False)\n",
    "print(\"Wrote to file!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d0978807fb3044ad9dd52c192bffca886a85a356693bc10d2667215d6e7d22f4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
